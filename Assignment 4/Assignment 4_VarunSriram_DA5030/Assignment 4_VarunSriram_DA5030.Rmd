---
title: "R Notebook"
name: Varun Sriram
output:
  html_document:
    df_print: paged
  html_notebook: default
assignment: DA 5030 Assignemnt 4
---


```{r, Question 1}
#Question 1
#reading the data from the .csv file

sms <- read.csv("sms.csv", header= TRUE, stringsAsFactors = FALSE)
colnames(sms) <- c("type", "text")

#Analysing the data
#str(sms)
#head(sms)

#Converting a chaaracter vector into a factor which will make it easier for computation. Also since type is a categorical variabel, it is a better approach to convert it to factor.
sms$type <- factor(sms$type)

#checking the table if the values are converted to factors
str(sms)


#DATA PREP PHASE - cleaning and standardizing the data
#Getting a count of hams and spams in the dataset
table(sms$type)

#importing library "tm". The tm library contains various 
library(tm)

#VCorpus() refers to volatice corpus, meaning that the data will be stored in the volatile memoery. It is nothing but a collection of .txt documents which will store text data. It is a collection of messages in this scenario. THe output is a list
sms_corpus <- VCorpus(VectorSource(sms$text))

#check the number of documents in the list
print(sms_corpus)

#Receive summary of a specific item in the list, we use inpect()
inspect(sms_corpus[1:2])

#Viewing an element of the list as an actual message.
as.character(sms_corpus[[1]])

#Viewing multiple elements as an actual message
lapply(sms_corpus[1:2], as.character)

#To standardize the texts for analysis, we apply the tolower() function to bring consistency in the texts
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))

#checking if tolower() workd
as.character(sms_corpus_clean[[1]])

#Numbers in text messages are mostly redundant. But they wont provide useful information regarding it.
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)

#to, and , but and or are caled stop words, which are generally present in both spam and ham messages. It will hinder the analysis. So we remove them using the stopwords() function
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())

#for the same reasons above, we remove punctuations using removepuncutations()
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)

#loading library snowball
library(SnowballC)

# The stemming process takes words like learned, learning, and learns, and strips the suffix in order to transform them into the base form, "learn""

#Stem standardinzing the data set
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)

#Removing white spaces generated by stem standardizing
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)

#tmpackage provides functionality to tokenize the SMS  corpus. The DocumentTermMatrix() function will take a corpus and create a data structure called a Document Term Matrix (DTM) in which rows indicate documents (SMS messages) and columns indicate terms (words).

sms_dtm <- DocumentTermMatrix(sms_corpus_clean)

sms_dtm


#Creating in training and testing data set
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test  <- sms_dtm[4170:5559, ]

#Getting the labels
sms_train_labels <- sms[1:4169, ]$type
sms_test_labels  <- sms[4170:5559, ]$type

#Get proportion of spam and ha
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))

#Import library, wordcloud for data visualization
library(wordcloud)

#Create word cloud with words having minimum frequency of 50 and with random order off. (Higher freq words will be at center)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)

#Creating subsets for ham and spam
spam <- subset(sms, type == "spam")

ham <- subset(sms, type == "ham")

#Creating wordclouf for spam and ham with min and max font size.
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))

#To eliminate words will low frequencies, we will not consider them. We create a new vector that will store all words with frequncy 5 or high
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)

str(sms_freq_words)

#Currently, our matrix has both text and words, we will extract the words only.
sms_dtm_freq_train<- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]


#Categorizing words by checking if they occur or not. (NAive bayes works way better on categorical values)
convert_counts <- function(x) 
{
  x <- ifelse(x > 0, "Yes", "No")
}

#The apply()function allows a function to be used on each of the rows or columns in a matrix. It uses a MARGIN parameter to specify either rows or columns. Here, we'll use MARGIN = 2, since we're interested in the columns (MARGIN = 1 is used for rows)
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)


library(e1071)

#Build model using naive bayes theorem
sms_classifier <- naiveBayes(sms_train, sms_train_labels)

#make predictions for spam and ham
sms_test_pred <- predict(sms_classifier, sms_test)


library(gmodels)
#compare predictions and get the accuracy
CrossTable(sms_test_pred, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))

#Laplace with add +1 occurences to all words so that, word with 0 occurences are no longer in the model. If there's a word with 0 frequency, the overall model results in a 0.
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 1)

#making predictions for spam and ham
sms_test_pred2 <- predict(sms_classifier2, sms_test)

#compare predictions with real values and get the accuracy
CrossTable(sms_test_pred2, sms_test_labels,prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE, dnn = c('predicted', 'actual'))

#As we can see, addition of laplace smoothing reduced the accuracy of our model. This is because there are no words with 0 frequency or occurences. The addition of laplace value (1) to all the word occurences changed the model's computation and reduced the accuracy.

```

```{r, Question 2}
#Question 2

#Importing klar library, which contains miscellaneous functions for classification and visualization. Importantly , It has function for NaiveBayes computation and visualization, which is required in this problem
library(klaR)

#Loads the specified data set
data(iris)

#Get the number of rows in the data set
nrow(iris)

#Get mathematical details of the data set iris
summary(iris)

#Print the first 6 rows of the data set
head(iris)

#Identify indexes in the data set and store them as testing data set. The indexes are multiples of 5 from the raw data set. Others are training data set values 
testidx <- which(1:length(iris[, 1]) %% 5 == 0)

#The data set stored in training data set -index = values other than training data set
iristrain <- iris[-testidx,]

#making the test data set
iristest <- iris[testidx,]

# apply Naive Bayes
nbmodel <- NaiveBayes(Species~., data=iristrain)

# check the accuracy
prediction <- predict(nbmodel, iristest[,-5])
table(prediction$class, iristest[,5])

#Question 1 : predict() Computes the conditional posterior probability of categorical class variables with given independent variables. It uses a specified amount of columns to predict and leaves one for prediction.

#Question 2 : In the function, The numeric features are considered, analyzed and compared with features of other numeric rows to classify the the test values into their respective species.

#Question 3 : the naiveBayes() function, by default, the function considers laplace estimator value = 0 unless specified.


```


```{r, Question 3}

#Question 3

#Let us consider the sms example. Once we've trained a Naive Bayes Algorithm, which contains a huge set of words, to differentiate between spam and ham. What if the word "Mercury" come up which is in the test data set and not training. Since the algorithm has never seen it before, it will set the probability to 0 ; therefore the probability of the whole doc to be spam is set to 0 too, even if there are other spam words like viagra or whatever. To circumvent the issues with unseen values, and to stop overfitting data, we add each occurence by 1 or by k, depending on your value of k more than occurence and adjust the denominator of our frequency divisions by the overall vocab.

```